{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from models.our_alexnet import AlexNet\n",
    "from models.inception import InceptionNet\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "\n",
    "try:\n",
    "    # Use __file__ if available\n",
    "    script_dir = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # Fallback to current working directory if __file__ is not available\n",
    "    # probably in jupyter notebook\n",
    "    script_dir = Path(\".\").resolve()\n",
    "\n",
    "script_dir = \"/om2/user/jackking/generalization-bounds\"\n",
    "\n",
    "# Step 1: Load the config.yaml file\n",
    "def load_config():\n",
    "    config_path = os.path.join(script_dir, \"configs/train_config.yaml\")\n",
    "    with open(config_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, valid_loader, loss_fn, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss += loss_fn(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            if labels.size(1) > 1:\n",
    "                #labels are onehot encoded\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    loss = loss / len(valid_loader)\n",
    "    return loss, accuracy\n",
    "\n",
    "def train(model, optimizer, loss_fn, lr_scheduler, reg_function, train_loader, valid_loader, num_epochs, run_name, save_epochs=10):\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        torch.cuda.empty_cache()\n",
    "        for step, (inputs, labels) in tqdm(enumerate(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            if reg_function is not None:\n",
    "                loss += reg_function(model)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            wandb.log({\"step\": step + len(train_loader) * epoch,\n",
    "                        \"train_loss\": loss.item(),\n",
    "                        \"learning_rate\": optimizer.param_groups[0]['lr']})\n",
    "            \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "        # evaluate\n",
    "        val_loss, val_accuracy = evaluate(model, valid_loader, loss_fn, device)\n",
    "        wandb.log({\"epoch\": epoch,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"val_accuracy\": val_accuracy})\n",
    "\n",
    "        #save model\n",
    "        if save_epochs != 0 and epoch % save_epochs == 0:\n",
    "            continue\n",
    "            #save logic\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    #save model\n",
    "\n",
    "def get_data_loaders(config):\n",
    "\n",
    "    ################################################\n",
    "    ##### get_datasets needs to be implemented #####\n",
    "    ################################################\n",
    "\n",
    "    # should return two tensors, train_dataset and valid_dataset\n",
    "    # should be normalized already\n",
    "    # train_dataset, valid_dataset = get_datasets(config[\"data\"])\n",
    "\n",
    "    #then delete this:\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "    valid_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    loss_fn = config[\"training\"][\"loss_fn\"]\n",
    "    #need to onehot encode labels for MSE loss    \n",
    "    if loss_fn == 'MSE':\n",
    "        train_dataset.targets = torch.nn.functional.one_hot(torch.tensor(train_dataset.targets), num_classes=config[\"data\"][\"num_classes\"]).float()\n",
    "        valid_dataset.targets = torch.nn.functional.one_hot(torch.tensor(valid_dataset.targets), num_classes=config[\"data\"][\"num_classes\"]).float()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
    "        \n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def get_loss_fn(config):\n",
    "    loss_fn = config[\"training\"][\"loss_fn\"]\n",
    "    if loss_fn == 'CE':\n",
    "        return nn.CrossEntropyLoss()\n",
    "    elif loss_fn == 'MSE':\n",
    "        return nn.MSELoss()\n",
    "    elif loss_fn == 'l1':\n",
    "        return nn.L1Loss()\n",
    "\n",
    "def get_lr_scheduler(config):\n",
    "    #NOTE: \"we do not need to change the learning rate schedule [for fitting random labels]\" (Zhang et al., 2017)\n",
    "    # however, for regular labels they have a decay factor of 0.95 every epoch\n",
    "    # They use an initial learning rate of 0.1 for Inception and 0.01 for AlexNet\n",
    "\n",
    "    lr_scheduler = config[\"training\"][\"lr_scheduler\"]\n",
    "    if lr_scheduler == 'StepLR':\n",
    "        return optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "    elif lr_scheduler == 'ReduceLROnPlateau':\n",
    "        return optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "    elif lr_scheduler == None:\n",
    "        return None\n",
    "    \n",
    "def get_optimizer(config):\n",
    "    optimizer = config[\"training\"][\"optimizer\"]\n",
    "    lr = config[\"training\"][\"learning_rate\"]\n",
    "    \n",
    "    if optimizer == 'Adam':\n",
    "        return optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer == 'SGD':\n",
    "        return optim.SGD(model.parameters(), lr=lr, momentum=config[\"training\"][\"momentum\"])\n",
    "    elif optimizer == 'RMSprop':\n",
    "        return optim.RMSprop(model.parameters(), lr=lr)\n",
    "\n",
    "def L2_reg(weight_decay):\n",
    "    def reg(model):\n",
    "        l2_reg = torch.tensor(0., requires_grad=True)\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                l2_reg = l2_reg + torch.norm(param, p=2)\n",
    "        return l2_reg * weight_decay\n",
    "    return reg\n",
    "\n",
    "def L1_reg(weight_decay):\n",
    "    def reg(model):\n",
    "        l1_reg = torch.tensor(0., requires_grad=True)\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                l1_reg = l1_reg + torch.norm(param, p=1)\n",
    "        return l1_reg * weight_decay\n",
    "    return reg\n",
    "\n",
    "def get_regularizer(config):\n",
    "    reg = config[\"training\"][\"regularization\"]\n",
    "    if reg == 'L1':\n",
    "        return L1_reg(config[\"training\"][\"weight_decay\"])\n",
    "    elif reg == 'L2':\n",
    "        return L2_reg(config[\"training\"][\"weight_decay\"])\n",
    "    elif reg == None:\n",
    "        return None\n",
    "\n",
    "def get_model(config):\n",
    "    if config[\"model\"][\"name\"] == \"AlexNet\":\n",
    "        return AlexNet(num_classes=config[\"data\"][\"num_classes\"])\n",
    "    elif config[\"model\"][\"name\"] == \"InceptionNet\":\n",
    "        return InceptionNet(num_classes=config[\"data\"][\"num_classes\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = load_config()\n",
    "\n",
    "    wandb.login(key=\"your_api_key_here\")\n",
    "\n",
    "    device = torch.device(config[\"training\"][\"device\"] if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader, valid_loader = get_data_loaders(config)\n",
    "    model = get_model(config).to(device)\n",
    "    loss_fn = get_loss_fn(config)\n",
    "    lr_scheduler = get_lr_scheduler(config)\n",
    "    reg_function = get_regularizer(config)\n",
    "    optimizer = get_optimizer(config)\n",
    "\n",
    "    num_epochs = config[\"training\"][\"num_epochs\"]\n",
    "    save_epochs = config[\"training\"][\"save_epochs\"]\n",
    "\n",
    "    wandb.init(project=\"generalization_bounds\", config=config)\n",
    "    run_name = wandb.run.name\n",
    "\n",
    "    train(model, optimizer, loss_fn, lr_scheduler, reg_function, train_loader, valid_loader, num_epochs, run_name, save_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

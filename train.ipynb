{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [00:12, 62.62it/s]:00<?, ?it/s]\n",
      "782it [00:12, 62.49it/s]:15<01:34, 15.69s/it]\n",
      "782it [00:12, 61.87it/s]:31<01:17, 15.58s/it]\n",
      "782it [00:12, 62.96it/s]:46<01:02, 15.64s/it]\n",
      "782it [00:12, 63.96it/s]:02<00:46, 15.66s/it]\n",
      "782it [00:12, 62.85it/s]:18<00:31, 15.62s/it]\n",
      "782it [00:12, 64.17it/s]:33<00:15, 15.55s/it]\n",
      "100%|██████████| 7/7 [01:49<00:00, 15.58s/it]\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "from data_utils import get_train_dataloader, get_test_dataloader\n",
    "import os \n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from models.our_alexnet import AlexNet\n",
    "from models.inception import InceptionNet\n",
    "import os\n",
    "\n",
    "\n",
    "try:\n",
    "    # Use __file__ if available\n",
    "    script_dir = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # Fallback to current working directory if __file__ is not available\n",
    "    # probably in jupyter notebook\n",
    "    script_dir = Path(\".\").resolve()\n",
    "\n",
    "# Step 1: Load the config.yaml file\n",
    "def load_config():\n",
    "    config_path = os.path.join(script_dir, \"configs/train_config.yaml\")\n",
    "    with open(config_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "script_dir = \"/om2/user/jackking/generalization-bounds\"\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, valid_loader, loss_fn, num_classes, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            #one hot encode labels for MSE loss\n",
    "            if len(labels.shape) == 1:\n",
    "                labels = torch.nn.functional.one_hot(labels, num_classes=num_classes).float()\n",
    "            loss += loss_fn(outputs, labels).item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    loss = loss / len(valid_loader)\n",
    "    return loss, accuracy\n",
    "\n",
    "def train(model, optimizer, loss_fn, lr_scheduler, reg_function, train_loader, valid_loader, num_epochs, num_classes, device, save_epochs=10):\n",
    "    last_5_train_accuracies = [0, 0, 0, 0, 0]\n",
    "    #make model directory\n",
    "    os.makedirs(script_dir + \"/saved_models/model\", exist_ok=True)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        torch.cuda.empty_cache()\n",
    "        for step, (inputs, labels) in tqdm(enumerate(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            #one hot encode labels for MSE loss\n",
    "            if len(labels.shape) == 1:\n",
    "                labels = torch.nn.functional.one_hot(labels, num_classes=num_classes).float()\n",
    "        \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            if reg_function is not None:\n",
    "                loss += reg_function(model)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        train_accuracy = correct / total\n",
    "            \n",
    "        # evaluate\n",
    "        val_loss, val_accuracy = evaluate(model, valid_loader, loss_fn, num_classes, device)\n",
    "\n",
    "        #save model if train accuracy is not increasing\n",
    "        if last_5_train_accuracies:\n",
    "            if train_accuracy <= np.mean(last_5_train_accuracies):\n",
    "                torch.save(model.state_dict(), script_dir + \"/saved_models/model/epoch_\" + str(epoch) + \".pt\")\n",
    "                last_5_train_accuracies = [] #don't save model again\n",
    "            else:\n",
    "                last_5_train_accuracies.pop(0)\n",
    "                last_5_train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    #save final model\n",
    "    torch.save(model.state_dict(), script_dir + \"/saved_models/model/final.pt\")\n",
    "\n",
    "    \n",
    "\n",
    "def get_data_loaders(config):\n",
    "    # should return two tensors, train_dataset and valid_dataset\n",
    "    # should be normalized already\n",
    "    train_loader = get_train_dataloader(dataset=config[\"data\"][\"dataset\"],\n",
    "                                        batch_size=config[\"training\"][\"batch_size\"], \n",
    "                                        loss_fn = config[\"training\"][\"loss_fn\"],\n",
    "                                        corruption_type=config[\"data\"][\"corruption_type\"],\n",
    "                                        corruption_prob=config[\"data\"][\"corruption_prob\"],\n",
    "                                        num_classes=config[\"data\"][\"num_classes\"],\n",
    "                                        num_workers=config[\"data\"][\"num_workers\"])\n",
    "    \n",
    "    test_loader = get_test_dataloader(dataset=config[\"data\"][\"dataset\"],\n",
    "                                        batch_size=config[\"training\"][\"batch_size\"], \n",
    "                                        loss_fn = config[\"training\"][\"loss_fn\"],\n",
    "                                        num_classes=config[\"data\"][\"num_classes\"],\n",
    "                                        num_workers=config[\"data\"][\"num_workers\"])\n",
    "\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def get_loss_fn(config):\n",
    "    loss_fn = config[\"training\"][\"loss_fn\"]\n",
    "    if loss_fn == 'CE':\n",
    "        return nn.CrossEntropyLoss()\n",
    "    elif loss_fn == 'MSE':\n",
    "        return nn.MSELoss()\n",
    "    elif loss_fn == 'l1':\n",
    "        return nn.L1Loss()\n",
    "\n",
    "def get_lr_scheduler(config, optimizer, step_size=1, gamma=0.95):\n",
    "    #NOTE: They have a decay factor of 0.95 every epoch\n",
    "    # They use an initial learning rate of 0.1 for Inception and 0.01 for AlexNet\n",
    "\n",
    "    lr_scheduler = config[\"training\"][\"lr_scheduler\"]\n",
    "    if lr_scheduler == 'StepLR':\n",
    "        return optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    elif lr_scheduler == 'ReduceLROnPlateau':\n",
    "        return optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "    elif lr_scheduler == None:\n",
    "        return None\n",
    "    \n",
    "def get_optimizer(config, model):\n",
    "    optimizer = config[\"training\"][\"optimizer\"]\n",
    "    lr = config[\"training\"][\"learning_rate\"]\n",
    "    \n",
    "    if optimizer == 'Adam':\n",
    "        return optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer == 'SGD':\n",
    "        return optim.SGD(model.parameters(), lr=lr, momentum=config[\"training\"][\"momentum\"])\n",
    "    elif optimizer == 'RMSprop':\n",
    "        return optim.RMSprop(model.parameters(), lr=lr)\n",
    "\n",
    "def L2_reg(weight_decay):\n",
    "    def reg(model):\n",
    "        l2_reg = torch.tensor(0., requires_grad=True)\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                l2_reg = l2_reg + torch.norm(param, p=2)\n",
    "        return l2_reg * weight_decay\n",
    "    return reg\n",
    "\n",
    "def L1_reg(weight_decay):\n",
    "    def reg(model):\n",
    "        l1_reg = torch.tensor(0., requires_grad=True)\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                l1_reg = l1_reg + torch.norm(param, p=1)\n",
    "        return l1_reg * weight_decay\n",
    "    return reg\n",
    "\n",
    "def get_regularizer(config):\n",
    "    reg = config[\"training\"][\"regularization\"]\n",
    "    if reg == 'L1':\n",
    "        return L1_reg(config[\"training\"][\"weight_decay\"])\n",
    "    elif reg == 'L2':\n",
    "        return L2_reg(config[\"training\"][\"weight_decay\"])\n",
    "    elif reg == None:\n",
    "        return None\n",
    "\n",
    "def get_model(config):\n",
    "    if config[\"model\"][\"name\"] == \"AlexNet\":\n",
    "        return AlexNet(num_classes=config[\"data\"][\"num_classes\"])\n",
    "    elif config[\"model\"][\"name\"] == \"InceptionNet\":\n",
    "        return InceptionNet(num_classes=config[\"data\"][\"num_classes\"])\n",
    "    \n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training function. Hydra automatically loads the configuration into `config`.\n",
    "    \"\"\"\n",
    "\n",
    "    config = load_config()\n",
    "\n",
    "    device = torch.device(config[\"training\"][\"device\"] if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    step_size = config[\"training\"][\"lr_scheduler_params\"][\"step_size\"]\n",
    "    gamma = config[\"training\"][\"lr_scheduler_params\"][\"gamma\"]\n",
    "\n",
    "    train_loader, valid_loader = get_data_loaders(config)\n",
    "    model = get_model(config).to(device)\n",
    "    loss_fn = get_loss_fn(config)\n",
    "    reg_function = get_regularizer(config)\n",
    "    optimizer = get_optimizer(config, model)\n",
    "    lr_scheduler = get_lr_scheduler(config, optimizer, step_size, gamma)\n",
    "\n",
    "    num_epochs = config[\"training\"][\"num_epochs\"]\n",
    "    num_epochs = 7\n",
    "    save_epochs = config[\"training\"][\"save_epochs\"]\n",
    "    num_classes = config[\"data\"][\"num_classes\"]\n",
    "\n",
    "    train(model, optimizer, loss_fn, lr_scheduler, reg_function, train_loader, valid_loader, num_epochs, num_classes, device, save_epochs)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
